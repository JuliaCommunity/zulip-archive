[
    {
        "content": "<p>I'm playing with a cache-oblivious algorithm (single-threaded) and seeing some puzzling behavior where smaller base case sizes (x-axis) produce:</p>\n<ul>\n<li>slower run-time (first row) which is bad and unexpected</li>\n<li>smaller L1 cache misses (second row) which is good and expected</li>\n<li>more backend stalls (third row)</li>\n</ul>\n<p>I guess the wall clock and the backend stall measurements agree but I don't know why.</p>\n<p>Any idea how to debug/fix this?</p>\n<p><a href=\"/user_uploads/7178/LLQXkRARjuc93n8Eujuk2uR_/image.png\">image.png</a></p>\n<div class=\"message_inline_image\"><a href=\"/user_uploads/7178/LLQXkRARjuc93n8Eujuk2uR_/image.png\" title=\"image.png\"><img src=\"/user_uploads/7178/LLQXkRARjuc93n8Eujuk2uR_/image.png\"></a></div>",
        "id": 263226584,
        "sender_full_name": "Takafumi Arakaki (tkf)",
        "timestamp": 1638316777
    },
    {
        "content": "<p>The algorithm I'm playing with is a cache-oblivious implementation of a stencil. You can find the code here: <a href=\"https://github.com/tkf/ToyStencils.jl/blob/main/src/trapezoid.jl\">https://github.com/tkf/ToyStencils.jl/blob/main/src/trapezoid.jl</a> If you are curious, you can run the benchmark with <code>make</code> at the root of the repository.</p>",
        "id": 263226743,
        "sender_full_name": "Takafumi Arakaki (tkf)",
        "timestamp": 1638316824
    },
    {
        "content": "<p>More plots of <code>perf</code> output <a href=\"https://vega.github.io/editor/#/gist/b71b202a14e0b7abd1518ab3339673aa/plot.vegalite\">https://vega.github.io/editor/#/gist/b71b202a14e0b7abd1518ab3339673aa/plot.vegalite</a></p>",
        "id": 263227191,
        "sender_full_name": "Takafumi Arakaki (tkf)",
        "timestamp": 1638317047
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"284680\">@chriselrod</span> <span class=\"user-mention\" data-user-id=\"269150\">@Mason Protter</span>  I wonder if you guys saw something like this?</p>",
        "id": 263227431,
        "sender_full_name": "Takafumi Arakaki (tkf)",
        "timestamp": 1638317167
    },
    {
        "content": "<p>I haven't run or looked at the code, but my assumption would be that smaller blocks at one level require more passes over the data at the next level. E.g., if you have 84 iterations, a block size of 7 would mean 12 passes, while a block size of 12 would mean only 7 repetitions.<br>\nBlocking == reuse, and larger blocks yield more reuse.<br>\nOf course, you'll hit a limit with cache sizes. When a block size gets large enough to start leading to cache evictions, then reuse (of the memory movement) starts to plummet.<br>\nSo there's some balance involved, but typically you'd want blocks to make use of most of the cache size.</p>\n<p>Although, I don't have much experience with cache oblivious algorithms, for which this probably rings less true.</p>",
        "id": 263253648,
        "sender_full_name": "chriselrod",
        "timestamp": 1638344184
    },
    {
        "content": "<p>When the block size (base case size) goes from 2^12 to 2^11, I see big a drop in the L1 miss. It's consistent with this machine's L1 size 32 KiB = 2^15 bytes = 2^12 <code>Float64</code>s. I need two blocks for one base case so it makes sense that 2^11 is just enough. So, I think it's consistent with your cache eviction argument? The puzzle is that the time is worse in base size = 2^11 than in 2^12.</p>",
        "id": 263392187,
        "sender_full_name": "Takafumi Arakaki (tkf)",
        "timestamp": 1638419897
    }
]